# coding:utf-8
import logging
import os
import logging.config
import time
import threading
from ConfigParser import ConfigParser
from bs4 import BeautifulSoup
from http_helper import get_html


__author__ = 'swarm'


logging.config.fileConfig('logging.conf')
# create logger
logger = logging.getLogger('Crawler')

config_filename = os.path.join('.', 'crawler_config.ini')
config = ConfigParser()
config.read(config_filename)


def get_phone_urls(init_url):
    """ get all the phones urls according to the init url
    :param init_url:
    """
    urls = []
    next_page_url = init_url
    while next_page_url:
        temp_urls, next_page_url = single_page_urls(next_page_url)
        urls.extend(temp_urls)
    return urls


def single_page_urls(url):
    single_page = get_html(url, proxy=True, cache=True)
    single_page_soup = BeautifulSoup(single_page, 'lxml')
    products = single_page_soup.findAll("tr", {"class": "product-listitem"})
    pre_str = "http://s.etao.com"
    app_str = "?tab=comment#J_detail_tabs_label"
    temp_urls = [pre_str + product.find("td", {"class": "cell2"}).a.get('href') + app_str for product in products]
    #todo: next_page_url to be finished, because page-next is generated by JavaScript
    next_page_html = single_page_soup.find("a", {"class": "page-next"})
    next_page_url = next_page_html.get('href') if next_page_html else None
    return temp_urls, next_page_url


def test_get_phone_urls():
    init_url = config.get('crawler', 'init_url')
    print 'init_url: ', init_url
    # phone_urls, next_url = single_page_urls(init_url)
    # print len(phone_urls)
    # print 'test single_page_urls...length of phone_urls is: ', len(phone_urls)
    # print 'single_page_url next_page_url is: ', next_url
    phone_urls = get_phone_urls(init_url)
    print 'test_get_phone_urls...length of phone_urls is: ', len(phone_urls)


class PhoneCrawler(threading.Thread):
    def __init__(self, num, phone_url):
        threading.Thread.__init__(self)
        self.num = num
        self.phone_url = phone_url
        pass

    def get_pos_neg_url(self, phone_url):
        """ get both positive comment url
                     and negative comment url
                     according to the phone url
        :param phone_url:
        """
        phone_html = get_html(phone_url, proxy=True)
        phone_soup = BeautifulSoup(phone_html)
        j_comment_div = phone_soup.find("div", {"id": "J_comment"})
        j_comment_url = j_comment_div.get('data-url')
        phone_comment_html = get_html(j_comment_url, proxy=True)
        phone_comment_soup = BeautifulSoup(phone_comment_html)
        phone_filter_div = phone_comment_soup.find("div", {"class": "comment-filter"})
        pos_url = neg_url = ''
        for child in phone_filter_div:
            if (u'好評' in child.text) or (u'好评' in child.text):
                pos_url = child.get('href')
            elif (u'差評' in child.text) or (u'差评' in child.text):
                neg_url = child.get('href')
            else:
                pass
        return pos_url, neg_url

    def visit_comment_page(self, comment_url):
        """ visit the comment url page
            UNTIL there is no more [next page]
        :param comment_url:
        """
        comment_html = get_html(comment_url, proxy=True)
        comment_soup = BeautifulSoup(comment_html)
        next_page_tag = comment_soup.find("a", {"class": "page-next"})
        next_page_url = None
        if next_page_tag:
            next_page_url = next_page_tag.get('href')
            logger.info('next_page_url: %s' % str(next_page_url))
            print next_page_url
        else:
            print 'NO MORE NEXT PAGE'
        return next_page_url

    def all_comment_pages(self, comment_url):
        count = 10
        while 1:
            if (not comment_url) or (0 == count):
                break
            comment_url = self.visit_comment_page(comment_url)
            count -= 1

    def test_pos_neg_url(self):
        phone_url = 'http://s.etao.com/item/8184790.html?spm=1002.8.0.50.VtduKf&sku=3call&tab=comment#J_detail_tabs_label'
        pos_url, neg_url = self.get_pos_neg_url(phone_url)
        print "pos_url: ", pos_url
        print "neg_url: ", neg_url

    def test_visit_comment(self):
        # all comments
        comment_url = "http://dianping.etao.com/popup-comment-list-8184790-1-0-1-1.htm?wordId="
        # all comments and page = 80
        # comment_url = "http://dianping.etao.com/popup-comment-list-8184790-1-0-1-1.htm?page=80&wordId="
        # the last page
        # comment_url = "http://dianping.etao.com/popup-comment-list-8184790-1-0-3-1.htm?wordId="
        # gbk wrong encode page
        # comment_url = "http://dianping.etao.com/popup-comment-list-8184790-1-0-3-1.htm?page=81&wordId="
        count = 10
        while 1:
            if (not comment_url) or (0 == count):
                break
            comment_url = self.visit_comment_page(comment_url)
            count -= 1

    def test(self):
        # self.test_pos_neg_url()
        # self.test_visit_comment()
        # logger.info('test logger, haha, LOL, hello logger world!')
        # phone_url = 'http://s.etao.com/item/8184790.html?spm=1002.8.0.50.VtduKf&sku=3call&tab=comment#J_detail_tabs_label'
        # pos_url, neg_url = self.get_pos_neg_url(phone_url)
        # print 'pos ' * 10
        # self.all_comment_pages(pos_url)
        # print 'neg ' * 10
        # self.all_comment_pages(neg_url)
        pass

    def run(self):
        print 'we are now at: ', self.phone_url
        pos_url, neg_url = self.get_pos_neg_url(self.phone_url)
        print 'pos ' * 10
        self.all_comment_pages(pos_url)
        print 'neg ' * 10
        self.all_comment_pages(neg_url)


def main():
    #@todo: main
    init_url = config.get('crawler', 'init_url')
    all_phone_urls = get_phone_urls(init_url)
    index = 0
    crawler_nums = 6
    while (index + crawler_nums - 1) < len(all_phone_urls):
        print '-' * 20
        print 'index: ', index
        a = PhoneCrawler(0, all_phone_urls[index+0])
        b = PhoneCrawler(1, all_phone_urls[index+1])
        c = PhoneCrawler(2, all_phone_urls[index+2])
        d = PhoneCrawler(3, all_phone_urls[index+3])
        e = PhoneCrawler(4, all_phone_urls[index+4])
        f = PhoneCrawler(5, all_phone_urls[index+5])
        a.start()
        b.start()
        c.start()
        d.start()
        e.start()
        f.start()
        a.join()
        b.join()
        c.join()
        d.join()
        e.join()
        f.join()
        index += crawler_nums


if __name__ == '__main__':
    main()
